# Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¬Ø§Ù…Ø¹ Fine-tuning Ù…Ø¯Ù„ VibeVoice-Realtime Ø¨Ø± Ø±ÙˆÛŒ Ø¯ÛŒØªØ§Ø³Øª ÙØ§Ø±Ø³ÛŒ

Ø§ÛŒÙ† Ø³Ù†Ø¯ Ø­Ø§ÙˆÛŒ ØªÙ…Ø§Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø² Ú©Ø¯Ø¨ÛŒØ³ VibeVoice Ø§Ø³Øª Ú©Ù‡ Ø¨Ø±Ø§ÛŒ fine-tuning Ø¨Ø± Ø±ÙˆÛŒ Ø¯ÛŒØªØ§Ø³Øª ÙØ§Ø±Ø³ÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒØ¯.

---

## Û±. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„ Realtime Ùˆ Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ

### Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ

#### Ø§Ù„Ù) Ú©Ù„Ø§Ø³ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡
```python
from vibevoice.modular.modeling_vibevoice_streaming import VibeVoiceStreamingModel
```

- **Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„**: `/vibevoice/modular/modeling_vibevoice_streaming.py`
- **ØªÙˆØ¶ÛŒØ­**: Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ø´Ø§Ù…Ù„ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§ØµÙ„ÛŒ Ù…Ø¯Ù„ Ø§Ø³Øª (Ø¨Ø¯ÙˆÙ† generation logic)

#### Ø¨) Ú©Ù„Ø§Ø³ Ù…Ø¯Ù„ Inference
```python
from vibevoice.modular.modeling_vibevoice_streaming_inference import VibeVoiceStreamingForConditionalGenerationInference
```

- **Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„**: `/vibevoice/modular/modeling_vibevoice_streaming_inference.py`
- **ØªÙˆØ¶ÛŒØ­**: Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ø´Ø§Ù…Ù„ Ù…Ù†Ø·Ù‚ generation Ùˆ inference Ø§Ø³Øª (Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ)

### Configuration
```python
from vibevoice.modular.configuration_vibevoice_streaming import VibeVoiceStreamingConfig
```

#### Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ù‡Ù… Configuration:
- `acoustic_vae_dim`: 64 (Ø§Ø¨Ø¹Ø§Ø¯ latent space Ø¨Ø±Ø§ÛŒ acoustic tokens)
- `tts_backbone_num_hidden_layers`: 20 (ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ TTS)
- `decoder_config`: Qwen2Config (configuration Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ Ù¾Ø§ÛŒÙ‡)
- `diffusion_head_config`: VibeVoiceDiffusionHeadConfig

---

## Û². Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Submoduleâ€ŒÙ‡Ø§ÛŒ Ø¢Ú©ÙˆØ³ØªÛŒÚ©/ØµÙˆØªÛŒ

Ù…Ø¯Ù„ `VibeVoiceStreamingModel` Ø¯Ø§Ø±Ø§ÛŒ Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øªâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø§Ø³Øª:

### Ø§Ù„Ù) Language Models

```python
# Lower Transformer layers (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ text encoding)
self.language_model = AutoModel.from_config(lm_config)
# ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§ÛŒÙ‡: num_hidden_layers - tts_backbone_num_hidden_layers

# Upper Transformer layers (Ø¨Ø±Ø§ÛŒ TTS generation)
self.tts_language_model = AutoModel.from_config(tts_lm_config)
# ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§ÛŒÙ‡: tts_backbone_num_hidden_layers (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 20)
```

**Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…**:
- `language_model.norm` Ø¨Ù‡ `nn.Identity()` ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯Ù‡ (Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯)
- Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ Ø§Ø² Qwen2 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯

### Ø¨) Acoustic Components

```python
# 1. Acoustic Tokenizer (Decoder)
self.acoustic_tokenizer = AutoModel.from_config(config.acoustic_tokenizer_config)
# Ú©Ù„Ø§Ø³: VibeVoiceAcousticTokenizerModel
# Ù†Ù‚Ø´: ØªØ¨Ø¯ÛŒÙ„ latent representations Ø¨Ù‡ waveform

# 2. Acoustic Connector
self.acoustic_connector = SpeechConnector(
    config.acoustic_vae_dim,  # 64
    lm_config.hidden_size
)
# Ù†Ù‚Ø´: ØªØ¨Ø¯ÛŒÙ„ acoustic features Ø¨Ù‡ hidden states Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ
```

### Ø¬) Diffusion Head

```python
# Prediction Head (Diffusion Model)
self.prediction_head = AutoModel.from_config(config.diffusion_head_config)
# Ú©Ù„Ø§Ø³: VibeVoiceDiffusionHead
# Ù†Ù‚Ø´: ØªÙˆÙ„ÛŒØ¯ acoustic latents Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² diffusion process

# Noise Scheduler
self.noise_scheduler = DPMSolverMultistepScheduler(
    num_train_timesteps=config.diffusion_head_config.ddpm_num_steps,  # 1000
    beta_schedule=config.diffusion_head_config.ddpm_beta_schedule,    # "cosine"
    prediction_type=config.diffusion_head_config.prediction_type      # "v_prediction"
)
```

### Ø¯) Ø³Ø§ÛŒØ± Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øªâ€ŒÙ‡Ø§

```python
# TTS Input Type Embeddings
self.tts_input_types = nn.Embedding(
    num_embeddings=2,
    embedding_dim=config.decoder_config.hidden_size
)

# Scaling factors (Ø¨Ø±Ø§ÛŒ normalization)
self.speech_scaling_factor = torch.tensor(float('nan'))  # buffer
self.speech_bias_factor = torch.tensor(float('nan'))     # buffer
```

---

## Û³. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Tokenizer/Processor Ø±Ø³Ù…ÛŒ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†

### Ú©Ù„Ø§Ø³ Processor

```python
from vibevoice.processor.vibevoice_streaming_processor import VibeVoiceStreamingProcessor
```

**Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„**: `/vibevoice/processor/vibevoice_streaming_processor.py`

### Ø§Ø³ØªÙØ§Ø¯Ù‡

```python
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
processor = VibeVoiceStreamingProcessor.from_pretrained(
    "microsoft/VibeVoice-Realtime-0.5B"
)

# Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆØ±ÙˆØ¯ÛŒ
inputs = processor.process_input_with_cached_prompt(
    text="Ù…ØªÙ† Ø´Ù…Ø§ Ø§ÛŒÙ†Ø¬Ø§",
    cached_prompt=all_prefilled_outputs,  # voice prompt embedding
    padding=True,
    return_tensors="pt",
    return_attention_mask=True,
)
```

### Tokenizer Ø¯Ø§Ø®Ù„ÛŒ

Processor Ø§Ø² ÛŒÚ©ÛŒ Ø§Ø² Ø§ÛŒÙ† tokenizer Ù‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:

```python
from vibevoice.modular.modular_vibevoice_text_tokenizer import (
    VibeVoiceTextTokenizer,       # Ù†Ø³Ø®Ù‡ Ù…Ø¹Ù…ÙˆÙ„ÛŒ
    VibeVoiceTextTokenizerFast    # Ù†Ø³Ø®Ù‡ Ø³Ø±ÛŒØ¹
)
```

**Ù¾ÛŒØ´â€ŒÙØ±Ø¶**: Tokenizer Ø§Ø² `Qwen/Qwen2.5-1.5B` Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯

### Ø®Ø±ÙˆØ¬ÛŒ Processor

```python
# inputs Ø´Ø§Ù…Ù„:
{
    'input_ids': torch.LongTensor,           # token IDs Ø¨Ø±Ø§ÛŒ language_model
    'attention_mask': torch.LongTensor,
    'tts_lm_input_ids': torch.LongTensor,   # token IDs Ø¨Ø±Ø§ÛŒ tts_language_model
    'tts_lm_attention_mask': torch.LongTensor,
    'tts_text_ids': torch.LongTensor,       # Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ TTS
    'speech_input_mask': torch.BoolTensor,  # Ù…Ø§Ø³Ú© Ø¨Ø±Ø§ÛŒ speech tokens
}
```

---

## Û´. Ú©Ø´Ù Ù…Ø³ÛŒØ± Audio â†’ Acoustic Tokens

### Acoustic Tokenizer Model

```python
from vibevoice.modular.modular_vibevoice_tokenizer import VibeVoiceAcousticTokenizerModel
```

**Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„**: `/vibevoice/modular/modular_vibevoice_tokenizer.py`

### Configuration

```python
from vibevoice.modular.configuration_vibevoice import VibeVoiceAcousticTokenizerConfig
```

**Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ**:
- `vae_dim`: 64 (Ø§Ø¨Ø¹Ø§Ø¯ latent space)
- `causal`: True
- `encoder_ratios`: [8, 5, 5, 4, 2, 2] (Ù†Ø±Ø® Ú©Ø§Ù‡Ø´ Ø¯Ø± encoder)
- `decoder_ratios`: [8, 5, 5, 4, 2, 2] (Ù†Ø±Ø® Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø± decoder)
- `channels`: 1 (mono audio)

### Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Inference

```python
# Decode: latents â†’ waveform
audio = model.model.acoustic_tokenizer.decode(
    latents,           # shape: [batch, vae_dim, time] ÛŒØ§ [batch, time, vae_dim]
    cache=cache,
    sample_indices=sample_indices,
    use_cache=use_cache
)
```

**Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…**: Ø§ÛŒÙ† Ù…Ø¯Ù„ ÙÙ‚Ø· **decoder** Ø¯Ø§Ø±Ø¯ (Ù†Ù‡ encoder). Ø¯Ø± trainingØŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯:
1. ÛŒÚ© encoder Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ audio Ø¨Ù‡ acoustic latents Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯
2. ÛŒØ§ Ø§Ø² pre-computed latents Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯

### Compression Ratio

```python
# Ø§Ø² processor:
speech_tok_compress_ratio = 3200  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶

# Ø¨Ù‡ Ø§ÛŒÙ† Ù…Ø¹Ù†ÛŒ Ú©Ù‡:
# Ø§Ú¯Ø± audio 24kHz Ø¨Ø§Ø´Ø¯:
# 24000 samples/sec Ã· 3200 = 7.5 tokens/sec
# Ø§ÛŒÙ† Ù‡Ù…Ø§Ù† "ultra-low frame rate of 7.5 Hz" Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± documentation Ø°Ú©Ø± Ø´Ø¯Ù‡
```

---

## Ûµ. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Training Code Ùˆ Loss Functions

### âš ï¸ Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…

**Ú©Ø¯ training Ø±Ø³Ù…ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ø±ÛŒÙ¾ÙˆØ²ÛŒØªÙˆØ±ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª**. Ø§ÛŒÙ† Ø±ÛŒÙ¾Ùˆ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ú©Ø¯ inference Ø§Ø³Øª.

Ø¨Ø§ Ø§ÛŒÙ† Ø­Ø§Ù„ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø§Ø² Ú©Ø¯ inferenceØŒ Ù…Ù†Ø·Ù‚ training Ø±Ø§ Ø§Ø³ØªÙ†Ø¨Ø§Ø· Ú©Ù†ÛŒÙ…:

### Ø§Ù„Ù) Diffusion Loss (Ø§Ø³ØªÙ†Ø¨Ø§Ø· Ø´Ø¯Ù‡)

Ø§Ø² ØªØ§Ø¨Ø¹ `sample_speech_tokens` Ø¯Ø± inference:

```python
def sample_speech_tokens(self, condition, neg_condition, cfg_scale=3.0):
    """
    Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ø² acoustic tokens Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² diffusion

    Args:
        condition: hidden states Ø§Ø² TTS LM (Ø´Ø±Ø· Ù…Ø«Ø¨Øª)
        neg_condition: hidden states Ø¨Ø±Ø§ÛŒ unconditional (Ø´Ø±Ø· Ù…Ù†ÙÛŒ)
        cfg_scale: Ù…Ù‚ÛŒØ§Ø³ classifier-free guidance
    """
    self.model.noise_scheduler.set_timesteps(self.ddpm_inference_steps)
    condition = torch.cat([condition, neg_condition], dim=0)

    # Ø´Ø±ÙˆØ¹ Ø§Ø² Ù†ÙˆÛŒØ² ØªØµØ§Ø¯ÙÛŒ
    speech = torch.randn(condition.shape[0], self.config.acoustic_vae_dim)

    # Ø­Ù„Ù‚Ù‡ denoising
    for t in self.model.noise_scheduler.timesteps:
        half = speech[: len(speech) // 2]
        combined = torch.cat([half, half], dim=0)

        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†ÙˆÛŒØ²
        eps = self.model.prediction_head(
            combined,
            t.repeat(combined.shape[0]),
            condition=condition
        )

        # Classifier-free guidance
        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)
        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)
        eps = torch.cat([half_eps, half_eps], dim=0)

        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ
        speech = self.model.noise_scheduler.step(eps, t, speech).prev_sample

    return speech[: len(speech) // 2]
```

### Ø¨) Training Loop (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ)

Ø¨Ø± Ø§Ø³Ø§Ø³ inference codeØŒ training loop Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯:

```python
# 1. Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§
text_inputs = processor.tokenizer(text, return_tensors="pt")
# ÙØ±Ø¶: Ø´Ù…Ø§ acoustic latents Ø§Ø² Ù‚Ø¨Ù„ Ø¯Ø§Ø±ÛŒØ¯ ÛŒØ§ Ø¨Ø§ encoder Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡â€ŒØ§ÛŒ Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒØ¯
target_acoustic_latents = your_acoustic_encoder(audio)  # shape: [B, T, 64]

# 2. Ú¯Ø°Ø± Ø§Ø² language models
lm_outputs = model.model.language_model(
    input_ids=text_inputs['input_ids'],
    attention_mask=text_inputs['attention_mask'],
)
hidden_states = lm_outputs.last_hidden_state

tts_lm_outputs = model.model.tts_language_model(
    inputs_embeds=hidden_states,  # ÛŒØ§ ØªØ±Ú©ÛŒØ¨ Ø¨Ø§ speech embeddings
    attention_mask=...,
)
condition = tts_lm_outputs.last_hidden_state  # [B, T, hidden_size]

# 3. Diffusion training (v-prediction)
# Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ timestep ØªØµØ§Ø¯ÙÛŒ
timesteps = torch.randint(
    0,
    model.config.diffusion_head_config.ddpm_num_steps,
    (batch_size,)
)

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†ÙˆÛŒØ² Ø¨Ù‡ target
noise = torch.randn_like(target_acoustic_latents)
noisy_latents = model.noise_scheduler.add_noise(
    target_acoustic_latents,
    noise,
    timesteps
)

# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ diffusion head
predicted = model.model.prediction_head(
    noisy_latents,
    timesteps,
    condition=condition
)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ loss
if model.config.diffusion_head_config.prediction_type == "v_prediction":
    # v-prediction: Ù…Ø¯Ù„ velocity Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    velocity = model.noise_scheduler.get_velocity(
        target_acoustic_latents,
        noise,
        timesteps
    )
    loss = F.mse_loss(predicted, velocity)
elif model.config.diffusion_head_config.prediction_type == "epsilon":
    # epsilon-prediction: Ù…Ø¯Ù„ Ù†ÙˆÛŒØ² Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    loss = F.mse_loss(predicted, noise)
else:
    raise ValueError(f"Unknown prediction type")

# 4. Backpropagation
loss.backward()
optimizer.step()
```

### Ø¬) Additional Losses (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)

Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø®ÙˆØ§Ù‡ÛŒØ¯ loss Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯:

```python
# 1. EOS Classifier Loss
eos_logits = model.tts_eos_classifier(hidden_states)
eos_labels = ...  # True Ø¨Ø±Ø§ÛŒ Ø¢Ø®Ø±ÛŒÙ† tokenØŒ False Ø¨Ø±Ø§ÛŒ Ø¨Ù‚ÛŒÙ‡
eos_loss = F.binary_cross_entropy_with_logits(eos_logits, eos_labels)

# 2. Reconstruction Loss (Ø§Ú¯Ø± acoustic encoder Ø¯Ø§Ø±ÛŒØ¯)
reconstructed = model.model.acoustic_tokenizer.decode(predicted_latents)
recon_loss = F.l1_loss(reconstructed, target_audio)

# Total loss
total_loss = diffusion_loss + 0.1 * eos_loss + 0.01 * recon_loss
```

---

## Û¶. ØªØ­Ù„ÛŒÙ„ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ùˆ ØªØ¹ÛŒÛŒÙ† Freeze Strategy

### Ø§Ù„Ù) ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§

Ù…Ø¯Ù„ VibeVoice-Realtime-0.5B Ø¯Ø§Ø±Ø§ÛŒ Ø­Ø¯ÙˆØ¯ **500 Ù…ÛŒÙ„ÛŒÙˆÙ†** Ù¾Ø§Ø±Ø§Ù…ØªØ± Ø§Ø³Øª.

ØªÙˆØ²ÛŒØ¹ ØªÙ‚Ø±ÛŒØ¨ÛŒ:
- Language Model (Qwen2): ~70% Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
- Acoustic Tokenizer Decoder: ~20% Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
- Diffusion Head: ~5% Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
- Connectors Ùˆ Ø³Ø§ÛŒØ±: ~5% Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§

### Ø¨) Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Freeze (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Û±: Ù…Ø­Ø§ÙØ¸Ù‡â€ŒÚ©Ø§Ø±Ø§Ù†Ù‡)

```python
for name, param in model.named_parameters():
    if any(k in name for k in [
        "model.language_model",           # Freeze: lower text encoder
        "model.acoustic_tokenizer",       # Freeze: audio decoder
    ]):
        param.requires_grad = False
    else:
        # Fine-tune:
        # - model.tts_language_model (upper TTS layers)
        # - model.acoustic_connector
        # - model.prediction_head (diffusion)
        # - model.tts_eos_classifier
        # - model.tts_input_types
        param.requires_grad = True
```

**Ù…Ø²Ø§ÛŒØ§**:
- Ø­ÙØ¸ Ø¯Ø§Ù†Ø´ text encoding
- Ø­ÙØ¸ Ú©ÛŒÙÛŒØª audio decoder

**Ù…Ø¹Ø§ÛŒØ¨**:
- Ù…Ù…Ú©Ù† Ø§Ø³Øª adaptation Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ú©Ø§Ù…Ù„ Ù†Ø¨Ø§Ø´Ø¯

### Ø¬) Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Freeze (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Û²: Ù…ØªØ¹Ø§Ø¯Ù„ Ø¨Ø§ LoRA)

```python
from peft import LoraConfig, get_peft_model

# Freeze Ù‡Ù…Ù‡ Ú†ÛŒØ²
for param in model.parameters():
    param.requires_grad = False

# Ø§Ø¹Ù…Ø§Ù„ LoRA ÙÙ‚Ø· Ø¨Ù‡ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ
lora_config = LoraConfig(
    r=16,                    # rank
    lora_alpha=32,
    target_modules=[
        # TTS language model attention layers
        "model.tts_language_model.layers.*.self_attn.q_proj",
        "model.tts_language_model.layers.*.self_attn.k_proj",
        "model.tts_language_model.layers.*.self_attn.v_proj",
        "model.tts_language_model.layers.*.self_attn.o_proj",
        # Diffusion head
        "model.prediction_head.*",
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# Fine-tune connectors Ø¨Ù‡ ØµÙˆØ±Øª full (Ø®Ø§Ø±Ø¬ Ø§Ø² LoRA)
for name, param in model.named_parameters():
    if "acoustic_connector" in name or "tts_eos_classifier" in name:
        param.requires_grad = True
```

**Ù…Ø²Ø§ÛŒØ§**:
- Ø­Ø§ÙØ¸Ù‡ Ùˆ Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ØªØ±
- Ú©Ø§Ù‡Ø´ overfitting
- Ù‚Ø§Ø¨Ù„ Ø§Ø¯ØºØ§Ù… Ø¨Ø§ Ù…Ø¯Ù„ Ø§ØµÙ„ÛŒ

### Ø¯) Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Freeze (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Û³: Ø¬Ø³ÙˆØ±Ø§Ù†Ù‡)

```python
# Freeze ÙÙ‚Ø· acoustic tokenizer
for name, param in model.named_parameters():
    if "model.acoustic_tokenizer" in name:
        param.requires_grad = False
    else:
        param.requires_grad = True
```

**Ù…Ø²Ø§ÛŒØ§**:
- Ø¨ÛŒØ´ØªØ±ÛŒÙ† adaptation Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
- ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ pronunciation patterns ÙØ§Ø±Ø³ÛŒ

**Ù…Ø¹Ø§ÛŒØ¨**:
- Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø²Ø±Ú¯â€ŒØªØ±
- Ø±ÛŒØ³Ú© overfitting
- Ø±ÛŒØ³Ú© catastrophic forgetting

### Ù‡) ØªÙˆØµÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ

**Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹**:
1. Ø§Ø² Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Û² (LoRA) Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒØ¯
2. Ø¨Ø§ 2000-5000 step ØªØ³Øª Ú©Ù†ÛŒØ¯
3. Ú©ÛŒÙÛŒØª audio Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯
4. Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²ØŒ Ø¨Ù‡ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Û± ÛŒØ§ Û³ Ø¨Ø±ÙˆÛŒØ¯

---

## Û·. Hyperparameter Ù‡Ø§

### Ø§Ù„Ù) Learning Rate

```python
# Ø¨Ø±Ø§ÛŒ LoRA
learning_rate = 1e-4  # ÛŒØ§ 5e-5

# Ø¨Ø±Ø§ÛŒ Full Fine-tuning
learning_rate = 5e-5  # ÛŒØ§ 1e-5
```

### Ø¨) Batch Size Ùˆ Gradient Accumulation

```python
batch_size = 2              # ÛŒØ§ 4ØŒ Ø¨Ø³ØªÙ‡ Ø¨Ù‡ GPU
gradient_accumulation_steps = 4  # ÛŒØ§ 8

# Effective batch size = batch_size Ã— gradient_accumulation_steps
# Ù…Ø«Ø§Ù„: 2 Ã— 4 = 8
```

### Ø¬) Audio Duration

```python
max_duration_seconds = 10   # Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹
# Ø¨Ø¹Ø¯Ø§Ù‹ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ 15 ÛŒØ§ 20 Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯
```

### Ø¯) Training Steps

```python
# Ø¨Ø±Ø§ÛŒ Proof of Concept
num_steps = 2000

# Ø¨Ø±Ø§ÛŒ training Ø¬Ø¯ÛŒ
num_steps = 10000  # ÛŒØ§ Ø¨ÛŒØ´ØªØ±
```

### Ù‡) Diffusion Parameters

```python
# Ø¯Ø± inference
ddpm_inference_steps = 5    # Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª
# ÛŒØ§
ddpm_inference_steps = 20   # Ø¨Ø±Ø§ÛŒ Ú©ÛŒÙÛŒØª

# Ø¯Ø± training
ddpm_num_steps = 1000       # ØªØ¹Ø¯Ø§Ø¯ timesteps (Ø«Ø§Ø¨Øª Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒØ¯)
```

### Ùˆ) CFG Scale

```python
cfg_scale = 1.5  # Ø¯Ø± inference
# Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø§Ù„Ø§ØªØ± = Ù¾Ø§ÛŒØ¨Ù†Ø¯ÛŒ Ø¨ÛŒØ´ØªØ± Ø¨Ù‡ Ù…ØªÙ†
# Ù…Ù‚Ø¯Ø§Ø± Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± = ØªÙ†ÙˆØ¹ Ø¨ÛŒØ´ØªØ±
```

### Ø²) Optimizer

```python
optimizer = torch.optim.AdamW(
    trainable_params,
    lr=learning_rate,
    betas=(0.9, 0.999),
    eps=1e-8,
    weight_decay=0.01
)
```

### Ø­) Scheduler

```python
from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=500,      # 10% Ø§Ø² total steps
    num_training_steps=num_steps
)
```

---

## Û¸. Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ Checklist Ù†Ù‡Ø§ÛŒÛŒ

### âœ… Ù‚Ø¨Ù„ Ø§Ø² Training

- [ ] Ø±ÛŒÙ¾Ùˆ Ø±Ø§ clone Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯
- [ ] Ø§Ø³Ú©Ø±ÛŒÙ¾Øª `inspect_realtime.py` Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ø±Ø¯Ù‡ Ùˆ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯
- [ ] Ø¯ÛŒØªØ§Ø³Øª ÙØ§Ø±Ø³ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯ (Ù…ØªÙ† + audio)
- [ ] acoustic encoder Ø¯Ø§Ø±ÛŒØ¯ ÛŒØ§ pre-computed latents (ÛŒØ§ Ø±ÙˆØ´ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ target)

### âœ… Import Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù…

```python
import torch
from vibevoice.modular.modeling_vibevoice_streaming_inference import (
    VibeVoiceStreamingForConditionalGenerationInference
)
from vibevoice.processor.vibevoice_streaming_processor import (
    VibeVoiceStreamingProcessor
)
from peft import LoraConfig, get_peft_model  # Ø§Ú¯Ø± Ø§Ø² LoRA Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯
```

### âœ… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„

```python
model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
    "microsoft/VibeVoice-Realtime-0.5B",
    torch_dtype=torch.bfloat16,
    device_map="cuda",
    attn_implementation="flash_attention_2",
)
processor = VibeVoiceStreamingProcessor.from_pretrained(
    "microsoft/VibeVoice-Realtime-0.5B"
)
```

### âœ… Ø§Ø¹Ù…Ø§Ù„ Freeze Strategy

```python
# Ù…Ø«Ø§Ù„: Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Û±
for name, param in model.named_parameters():
    if "language_model" in name or "acoustic_tokenizer" in name:
        param.requires_grad = False
    else:
        param.requires_grad = True

# Ø¨Ø±Ø±Ø³ÛŒ
trainable = [n for n, p in model.named_parameters() if p.requires_grad]
print(f"Trainable parameters: {len(trainable)}")
```

### âœ… Ø³Ø§Ø®Øª Training Loop

```python
# TODO: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø®Ø´ Ûµ
```

### âœ… Inference Ùˆ ØªØ³Øª

```python
# Ø¨Ø¹Ø¯ Ø§Ø² trainingØŒ ØªØ³Øª Ú©Ù†ÛŒØ¯:
model.eval()
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=None,
        cfg_scale=1.5,
        tokenizer=processor.tokenizer,
    )
    processor.save_audio(outputs.speech_outputs[0], "output.wav")
```

### âœ… Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ

- Ú¯ÙˆØ´ Ø¯Ø§Ø¯Ù† Ø¨Ù‡ Ø®Ø±ÙˆØ¬ÛŒ (Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ†!)
- WER (Word Error Rate) Ø¨Ø§ ÛŒÚ© ASR model
- Speaker Similarity (Ø§Ú¯Ø± voice cloning Ø¯Ø§Ø±ÛŒØ¯)
- Naturalness MOS (Mean Opinion Score)

---

## Û¹. Ù†Ú©Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ

### Ø§Ù„Ù) Voice Prompts

Ù…Ø¯Ù„ Ù†ÛŒØ§Ø² Ø¨Ù‡ **cached voice prompt** Ø¯Ø§Ø±Ø¯ (embedded format).

Ø¯Ø± Ø¯Ù…ÙˆØŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ `.pt` Ø¯Ø± `demo/voices/streaming_model/` Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯:

```python
voice_sample = torch.load("demo/voices/streaming_model/Carter.pt")
all_prefilled_outputs = voice_sample

inputs = processor.process_input_with_cached_prompt(
    text=text,
    cached_prompt=all_prefilled_outputs,
    ...
)
```

**Ø¨Ø±Ø§ÛŒ training**: Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ voice prompts ÙØ§Ø±Ø³ÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯ ÛŒØ§ Ø§Ø² Ù…ÙˆØ¬ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.

### Ø¨) Text Normalization

Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒØŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒØ¯:
- Ø§Ø¹Ø¯Ø§Ø¯ Ø±Ø§ Ø¨Ù‡ Ø­Ø±ÙˆÙ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯
- Ø¹Ù„Ø§Ø¦Ù… Ø®Ø§Øµ Ø±Ø§ normalize Ú©Ù†ÛŒØ¯
- Ú©Ù„Ù…Ø§Øª Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ ØªØ¨Ø¯ÛŒÙ„ ÛŒØ§ transliterate Ú©Ù†ÛŒØ¯

### Ø¬) Audio Preprocessing

```python
# Sample rate Ø¨Ø§ÛŒØ¯ 24000 Hz Ø¨Ø§Ø´Ø¯
target_sample_rate = 24000

# Audio normalization
# processor.audio_processor Ø¯Ø§Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø³Øª:
# - normalize_audio = True
# - target_dB_FS = -25
```

### Ø¯) Streaming vs Non-streaming

Ù…Ø¯Ù„ realtime Ø¨Ø±Ø§ÛŒ **streaming text input** Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ØŒ Ø§Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø¢Ù† Ø¨Ø±Ø§ÛŒ non-streaming Ù‡Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.

Ø¯Ø± trainingØŒ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ non-streaming Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ø§Ø³Øª.

---

## Û±Û°. Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ù…Ø±Ø§Ø¬Ø¹

### Ú©Ø¯Ù‡Ø§ÛŒ Ù…Ø±Ø¬Ø¹

- **Ù…Ø¯Ù„ Ø§ØµÙ„ÛŒ**: `vibevoice/modular/modeling_vibevoice_streaming.py`
- **Inference**: `vibevoice/modular/modeling_vibevoice_streaming_inference.py`
- **Processor**: `vibevoice/processor/vibevoice_streaming_processor.py`
- **Diffusion Head**: `vibevoice/modular/modular_vibevoice_diffusion_head.py`
- **Acoustic Tokenizer**: `vibevoice/modular/modular_vibevoice_tokenizer.py`
- **Configuration**: `vibevoice/modular/configuration_vibevoice_streaming.py`

### Ø¯Ù…Ùˆ Ùˆ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§

- **Inference Ø§Ø² ÙØ§ÛŒÙ„**: `demo/realtime_model_inference_from_file.py`
- **WebSocket Demo**: `demo/vibevoice_realtime_demo.py`
- **Colab**: [vibevoice_realtime_colab.ipynb](https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb)

### Documentation

- **README**: `README.md`
- **Realtime Docs**: `docs/vibevoice-realtime-0.5b.md`
- **Technical Report**: https://arxiv.org/pdf/2508.19205

### Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ÛŒ Ú©Ù…Ú©ÛŒ

- **Inspection Script**: `inspect_realtime.py` (Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ø§ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§)

---

## Ù¾Ø§ÛŒØ§Ù†

Ø§ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§ ØªÙ…Ø§Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ fine-tuning Ù…Ø¯Ù„ VibeVoice-Realtime Ø±Ø§ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

**Ù…Ø±Ø§Ø­Ù„ Ø¨Ø¹Ø¯ÛŒ**:
1. Ø§Ø³Ú©Ø±ÛŒÙ¾Øª `inspect_realtime.py` Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯
2. Ø¯ÛŒØªØ§Ø³Øª Ø®ÙˆØ¯ Ø±Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
3. ÛŒÚ© training loop Ø³Ø§Ø¯Ù‡ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯
4. Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù… step ØªØ³Øª Ú©Ù†ÛŒØ¯
5. Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†ÛŒØ¯
6. hyperparameter Ù‡Ø§ Ø±Ø§ tune Ú©Ù†ÛŒØ¯
7. training Ú©Ø§Ù…Ù„ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯

**Ù…ÙˆÙÙ‚ Ø¨Ø§Ø´ÛŒØ¯!** ğŸ™ï¸ğŸ‡®ğŸ‡·
