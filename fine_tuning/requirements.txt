# Fine-tuning requirements for Qwen2.5-1.5B
# Optimized for RunPod with A40/A100 GPUs

# Core ML
torch>=2.1.0
transformers>=4.44.0
datasets>=2.14.0
accelerate>=0.24.0

# Unsloth - 2-5x faster fine-tuning, 80% less memory
unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git

# LoRA/PEFT
peft>=0.7.0
bitsandbytes>=0.41.0
trl>=0.7.0

# Data processing
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0

# Web scraping for automobile data
requests>=2.31.0
beautifulsoup4>=4.12.0
praw>=7.7.0  # Reddit API

# HuggingFace utilities
huggingface_hub>=0.19.0

# Progress and logging
tqdm>=4.66.0
wandb>=0.16.0  # Optional: for experiment tracking

# Evaluation
rouge-score>=0.1.2
nltk>=3.8.0
