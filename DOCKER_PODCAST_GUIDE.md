# VibeVoiceæ–°é—»æ’­å®¢Dockerä½¿ç”¨æŒ‡å—

## ğŸ³ å¿«é€Ÿå¼€å§‹

### 1. æ„å»ºå¹¶å¯åŠ¨å®¹å™¨

```bash
cd /path/to/VibeVoice/docker
docker compose up --build -d
```

### 2. è¿›å…¥å®¹å™¨

```bash
docker compose exec vibevoice bash
```

### 3. è¿è¡Œå®¹å™¨è®¾ç½®æ£€æŸ¥

```bash
./setup_container.sh
```

## ğŸ“» ç”Ÿæˆæ–°é—»æ’­å®¢

### åŸºç¡€ç”¨æ³•

```bash
# æµ‹è¯•ç®¡é“ï¼ˆä»…ç”Ÿæˆæ–‡æœ¬ï¼Œä¸ç”ŸæˆéŸ³é¢‘ï¼‰
python test_news_pipeline.py

# ç”Ÿæˆå®Œæ•´æ’­å®¢ï¼ˆåŒ…å«éŸ³é¢‘ï¼‰
python generate_news_podcast.py
```

### è‡ªå®šä¹‰å‚æ•°

```bash
# ä½¿ç”¨3ä¸ªè¯´è¯äºº
python generate_news_podcast.py --speakers 3

# ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
python generate_news_podcast.py --model-path WestZhang/VibeVoice-Large-pt

# è‡ªå®šä¹‰è¾“å‡ºç›®å½•
python generate_news_podcast.py --output-dir /app/podcast_output

# è·å–æ›´å¤šæ–°é—»
python generate_news_podcast.py --news-limit 20 --max-news-items 15

# æŸ¥çœ‹å¯ç”¨è¯­éŸ³
python generate_news_podcast.py --list-voices
```

### å®Œæ•´å‚æ•°ç¤ºä¾‹

```bash
python generate_news_podcast.py \
    --model-path microsoft/VibeVoice-1.5B \
    --speakers 4 \
    --news-limit 25 \
    --max-news-items 12 \
    --output-dir /app/podcast_output
```

## ğŸ“ æ–‡ä»¶è®¿é—®

ç”Ÿæˆçš„æ’­å®¢æ–‡ä»¶å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¿é—®ï¼š

### ä»å®¹å™¨å†…éƒ¨
```bash
ls /app/podcast_output/
```

### ä»å®¿ä¸»æœº
```bash
ls ./docker/podcast_output/
```

### ç”Ÿæˆçš„æ–‡ä»¶æ ¼å¼
- `news_podcast_YYYYMMDD_HHMMSS.wav` - æœ€ç»ˆéŸ³é¢‘æ–‡ä»¶
- `news_podcast_YYYYMMDD_HHMMSS_dialogue.txt` - å¯¹è¯è„šæœ¬
- `news_podcast_YYYYMMDD_HHMMSS_news.json` - åŸå§‹æ–°é—»æ•°æ®

## ğŸ”§ ç¯å¢ƒé…ç½®

å®¹å™¨é¢„è®¾äº†ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

```bash
MODEL_PATH=microsoft/VibeVoice-1.5B
OLLAMA_URL=http://172.36.237.245:11434
OLLAMA_MODEL=qwen2.5-coder:1.5b
```

### ä¿®æ”¹é…ç½®

å¯ä»¥åœ¨`docker-compose.yml`ä¸­ä¿®æ”¹è¿™äº›è®¾ç½®ï¼š

```yaml
environment:
  - MODEL_PATH=WestZhang/VibeVoice-Large-pt
  - OLLAMA_URL=http://your-ollama-server:11434
  - OLLAMA_MODEL=llama3.1
```

## ğŸ› æ•…éšœæ’é™¤

### 1. Ollamaè¿æ¥é—®é¢˜

```bash
# æ£€æŸ¥Ollamaè¿æ¥
curl http://172.36.237.245:11434/api/version

# åœ¨å®¹å™¨å†…æµ‹è¯•è¿æ¥
python -c "
from news_podcast.ollama_processor import OllamaProcessor
processor = OllamaProcessor()
print('âœ“ Connected' if processor.check_ollama_connection() else 'âœ— Failed')
"
```

### 2. GPUä¸å¯ç”¨

```bash
# æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi

# åœ¨å®¹å™¨å†…æ£€æŸ¥
python -c "import torch; print('GPU:', torch.cuda.is_available())"
```

### 3. å†…å­˜ä¸è¶³

- ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼š`microsoft/VibeVoice-1.5B`
- å‡å°‘è¯´è¯äººæ•°é‡ï¼š`--speakers 2`
- å‡å°‘æ–°é—»æ•°é‡ï¼š`--news-limit 10`

### 4. éŸ³é¢‘ç”Ÿæˆå¤±è´¥

```bash
# æ£€æŸ¥soundfileå®‰è£…
python -c "import soundfile; print('âœ“ soundfile available')"

# æ£€æŸ¥è¯­éŸ³æ–‡ä»¶
ls /app/demo/voices/en-*.wav
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### GPUå†…å­˜ä¼˜åŒ–
```bash
# ä½¿ç”¨1.5Bæ¨¡å‹è€Œä¸æ˜¯7B
python generate_news_podcast.py --model-path microsoft/VibeVoice-1.5B

# å‡å°‘å¹¶è¡Œå¤„ç†
python generate_news_podcast.py --speakers 2
```

### ç½‘ç»œä¼˜åŒ–
```bash
# å‡å°‘æ–°é—»è·å–æ•°é‡
python generate_news_podcast.py --news-limit 10 --max-news-items 6
```

## ğŸ”„ å®¹å™¨ç®¡ç†

```bash
# æŸ¥çœ‹å®¹å™¨çŠ¶æ€
docker compose ps

# æŸ¥çœ‹æ—¥å¿—
docker compose logs -f

# é‡å¯å®¹å™¨
docker compose restart

# åœæ­¢å®¹å™¨
docker compose down

# é‡æ–°æ„å»º
docker compose up --build
```

## ğŸ¯ ç¤ºä¾‹å·¥ä½œæµç¨‹

```bash
# 1. å¯åŠ¨å®¹å™¨
cd docker && docker compose up -d

# 2. è¿›å…¥å®¹å™¨
docker compose exec vibevoice bash

# 3. è¿è¡Œè®¾ç½®æ£€æŸ¥
./setup_container.sh

# 4. å¿«é€Ÿæµ‹è¯•
python test_news_pipeline.py

# 5. ç”Ÿæˆæ’­å®¢
python generate_news_podcast.py --speakers 3

# 6. æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶
ls /app/podcast_output/

# 7. ä»å®¿ä¸»æœºè®¿é—®æ–‡ä»¶
exit
ls ./podcast_output/
```

è¿™æ ·ä½ å°±å¯ä»¥åœ¨å®¹å™¨ä¸­å®Œæ•´åœ°è¿è¡Œæ–°é—»æ’­å®¢ç”ŸæˆåŠŸèƒ½äº†ï¼